{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edb8bbbf",
   "metadata": {},
   "source": [
    "# RAG From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a176a4a1",
   "metadata": {},
   "source": [
    "![RAG](rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04112334",
   "metadata": {},
   "source": [
    "`1. Packages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec99efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d5c03",
   "metadata": {},
   "source": [
    "`2. API KEYS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f37d341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608fdc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = LANGCHAIN_API_KEY\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40617e92",
   "metadata": {},
   "source": [
    "# Part 1: Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb7c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 # BeautifulSoup library for web scraping and parsing HTML/XML documents\n",
    "from langchain import hub # Library for accessing and managing various modules in LangChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # for splitting text into smaller chunks for processing\n",
    "from langchain_community.document_loaders import WebBaseLoader # Community-maintained library for loading web-based documents\n",
    "from langchain_community.vectorstores import Chroma # Community-maintained library for creating and managing vector stores\n",
    "from langchain_core.output_parsers import StrOutputParser # Core library for parsing output in LangChain\n",
    "from langchain_core.runnables import RunnablePassthrough # Core library for creating pass-through runnables in LangChain\n",
    "from langchain_openai import ChatOpenAI # Library for using OpenAI's ChatGPT models in LangChain\n",
    "from langchain_openai import OpenAIEmbeddings # Library for generating and managing OpenAI embeddings in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9c384a",
   "metadata": {},
   "source": [
    "### \\#### INDEXING \\#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704e681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ffb315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49efbfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3147849a",
   "metadata": {},
   "source": [
    "#### \\#### RETRIEVAL and GENERATION \\####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0bdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37503ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a621c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1209e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4678c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ed7fe",
   "metadata": {},
   "source": [
    "__Everything we have done above in a single code cell__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6528480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "#### INDEXING #### \n",
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dcc62b",
   "metadata": {},
   "source": [
    "# Part 2: Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e86e49",
   "metadata": {},
   "source": [
    "![title](indexing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9340f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents\n",
    "document = \"My favourite pet is a cat.\"\n",
    "question = \"What kinds of pets do I like?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd8289",
   "metadata": {},
   "source": [
    "How to count tokens using tiktoken ? <br>\n",
    "__https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d8950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d60e04",
   "metadata": {},
   "source": [
    "[__Text Embedding Models__](https://python.langchain.com/v0.2/docs/integrations/text_embedding/openai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a8a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embd = OpenAIEmbeddings()\n",
    "query_result = embd.embed_query(question)\n",
    "document_result = embd.embed_query(document)\n",
    "len(query_result), len(document_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda2d9fe",
   "metadata": {},
   "source": [
    "[Cosine Similarity](https://platform.openai.com/docs/guides/embeddings/use-cases) is recommended for OpenAI embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56707e0",
   "metadata": {},
   "source": [
    "__Why we calculated the cosine similarity between query and document ?__ <br>\n",
    "When you have a query and you want to find the most relevant document or response from a set of documents, you:\n",
    "\n",
    "1. Embed the Query and Documents: Convert the query and each document into vector representations.\n",
    "2. Calculate Similarity: Compute the cosine similarity between the query vector and each document vector.\n",
    "3. Rank Results: Rank the documents based on their similarity scores. The document with the highest similarity score is considered the most relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c2d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity: \", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66a59c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# Load blog\n",
    "import bs4\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9023cb17",
   "metadata": {},
   "source": [
    "[__Splitter__](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/)\n",
    "> This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71eb1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50)\n",
    "\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11154d38",
   "metadata": {},
   "source": [
    "# Part 3: Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b85d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1}) # returns nearest 1 vector (document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c850ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c48ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs) # as k=1 we get only 1 doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19650dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa53bd21",
   "metadata": {},
   "source": [
    "# Part 4: Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea61635",
   "metadata": {},
   "source": [
    "![title](generation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb8e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "              {context}\n",
    "              Question: {question}\n",
    "           \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05a353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4233e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cebd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run \n",
    "chain.invoke({\"context\":docs, \"question\":\"What is Task Decomposition\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42788a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt_hub_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b594e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
